# -*- coding: utf-8 -*-
"""Daily News for Stock Market Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ENOH-XctFICnbrX4pzhRR7ltkYBaIcQU
"""

import shutil

# สร้างโฟลเดอร์ .kaggle และย้ายไฟล์ไปไว้ที่นั่น
# !mkdir -p ~/.kaggle
# !mv kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json  # ตั้งค่า permission

# ติดตั้ง Kaggle CLI (ถ้ายังไม่ได้ติดตั้ง)
# !pip install kaggle

# ดาวน์โหลด dataset
# !kaggle datasets download -d aaron7sun/stocknews

# แตกไฟล์ ZIP ออกมา
# !unzip stocknews.zip

import pandas as pd

# โหลดข่าวที่ต้องการใช้
news_df = pd.read_csv("./Data/Combined_News_DJIA.csv")  # หรือ "RedditNews.csv"

# ดูตัวอย่างข้อมูล
print(news_df.head())

import yfinance as yf

# เลือกดัชนีหุ้นที่ต้องการ (เช่น Dow Jones)
ticker = "^DJI"

# ดึงข้อมูลย้อนหลัง 10 ปี
stock_df = yf.download(ticker, start="2013-01-01", end="2023-12-31")

# ตรวจสอบคอลัมน์ที่มีอยู่
print(stock_df.columns)

# รีเซ็ต Index เพื่อให้มี Date เป็นคอลัมน์
stock_df.reset_index(inplace=True)

# เลือกเฉพาะคอลัมน์ที่ต้องใช้
stock_df = stock_df[['Date', 'Close']]  # ใช้ 'Close' แทน 'Adj Close'
stock_df.rename(columns={'Close': 'Stock_Price'}, inplace=True)

# บันทึกเป็น stock_prices.csv
stock_df.to_csv("stock_prices.csv", index=False)

# แสดงตัวอย่างข้อมูล
print(stock_df.head())

# โหลดข้อมูลที่สร้างขึ้น
news_df = pd.read_csv("Combined_News_DJIA.csv")
stock_df = pd.read_csv("stock_prices.csv")

# ตรวจสอบว่าคอลัมน์ Date อยู่ในรูปแบบ datetime
news_df["Date"] = pd.to_datetime(news_df["Date"])
stock_df["Date"] = pd.to_datetime(stock_df["Date"])

# รวมข้อมูลข่าวและราคาหุ้น
df = pd.merge(news_df, stock_df, on="Date")

# แสดงตัวอย่างข้อมูลที่รวมกันแล้ว
print(df.head())

"""# ทำ Data **Preprocessing**

# ทำความสะอาดข้อมูล (Text Cleaning)

ลบ Stopwords (เช่น "the", "is", "and")
แปลงเป็น Lowercase
ใช้ Lemmatization เพื่อลดคำซ้ำ
ลบสัญลักษณ์และตัวเลขที่ไม่จำเป็น **bold text**
"""

# เช็ค Columns ว่ามีอะไรบ้างก่อน
print("Available columns in df:", df.columns)

# รวม Top1 ถึง Top25 เป็น "News_Headline"
df['News_Headline'] = df[['Top'+str(i) for i in range(1, 26)]].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)

print(df[['Date', 'News_Headline']].head())  # แสดงตัวอย่าง 5 แถวแรก

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# ดาวน์โหลด resource ที่จำเป็น
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')  # สำหรับ Lemmatizer
# nltk.download('punkt') เสีย!!!


# ตั้งค่า lemmatizer และ stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# ฟังก์ชันทำความสะอาดข้อความ
def clean_text(text):
    if isinstance(text, str):
        words = text.lower().split()  # ใช้ split() แทน word_tokenize()
        words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]
        return " ".join(words)
    return ""


# ถ้า News_Headline มีอยู่ → สร้าง Cleaned_News
if 'News_Headline' in df.columns:
    df['News_Headline'] = df['News_Headline'].fillna("")  # แทนค่า NaN
    df['Cleaned_News'] = df['News_Headline'].apply(clean_text)

    # ตรวจสอบว่าถูกสร้างหรือยัง
    print("Created 'Cleaned_News' successfully! Sample data:")
    print(df[['News_Headline', 'Cleaned_News']].head())
else:
    print("Error: 'News_Headline' column not found.")

"""# 2. แปลงข้อความเป็นตัวเลข (Text Vectorization)

ใช้ TF-IDF / Word2Vec / BERT เพื่อแปลงข้อความเป็นเวกเตอร์
ใช้ TfidfVectorizer() เพื่อแปลงข่าวให้โมเดลเข้าใจ
"""

# ไว้เช็คค่า Columms ว่ามีไหม ?
print("Available columns:", df.columns)
if 'Cleaned_News' not in df.columns:
    print("Error: 'Cleaned_News' column not found!")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X_text = vectorizer.fit_transform(df['Cleaned_News']).toarray()

print("TF-IDF Transformation Successful! Shape:", X_text.shape)

"""# 3. แปลง Target เป็น 0 (ลง) / 1 (ขึ้น)"""

if 'Stock_Price' in df.columns:
    # แปลง Stock_Price ให้เป็น float (ถ้ามีเครื่องหมาย $ หรือ , ให้ลบออกก่อน)
    df['Stock_Price'] = df['Stock_Price'].replace('[\$,]', '', regex=True).astype(float)

    # สร้าง Change โดยใช้ความต่างรายวัน
    df['Change'] = df['Stock_Price'].diff()

    # แทนค่า NaN ของวันแรกด้วย 0
    df['Change'] = df['Change'].fillna(0)

    # ตรวจสอบผลลัพธ์
    print(df[['Date', 'Stock_Price', 'Change']].head())
else:
    print("Error: 'Stock_Price' column not found in df!")

df['Stock_Price'] = pd.to_numeric(df['Stock_Price'], errors='coerce')
df['Stock_Price'] = df['Stock_Price'].fillna(0)  # ถ้ามีค่า NaN ให้แทนด้วย 0

df['Market_Movement'] = df['Change'].apply(lambda x: 1 if x > 0 else 0)

"""# Machine Learning

Logistic Regression
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X_text, df['Market_Movement'], test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
preds = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, preds))

"""Random Forest และ XGBoost"""

from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, rf_preds))
print("XGBoost Accuracy:", accuracy_score(y_test, xgb_preds))

"""# ปรับโครงสร้าง LSTM
ลองเพิ่ม Bidirectional LSTM + Dropout เพื่อลด Underfitting

เพิ่ม Feature อื่น ๆ (Sentiment + Technical Indicators)
โมเดลอาจเรียนรู้จากข่าวเพียงอย่างเดียวไม่ได้
"""

from textblob import TextBlob

# คำนวณ Sentiment Score และเพิ่มลงใน DataFrame
df['Sentiment'] = df['Cleaned_News'].apply(lambda x: TextBlob(x).sentiment.polarity)

# ตรวจสอบค่าที่ได้
print(df[['News_Headline', 'Sentiment']].head())

"""คำนวณ Technical Indicators"""

print("Missing values in Stock_Price:", df['Stock_Price'].isna().sum())

df['Stock_Price'] = df['Stock_Price'].fillna(method='ffill')  # ใช้ค่าก่อนหน้าเติมเต็ม

import ta

# คำนวณ Moving Averages ใหม่
df['SMA_50'] = ta.trend.SMAIndicator(close=df['Stock_Price'], window=50, fillna=True).sma_indicator()
df['SMA_200'] = ta.trend.SMAIndicator(close=df['Stock_Price'], window=200, fillna=True).sma_indicator()
df['EMA_50'] = ta.trend.EMAIndicator(close=df['Stock_Price'], window=50, fillna=True).ema_indicator()

# คำนวณ MACD (Moving Average Convergence Divergence)
df['MACD'] = ta.trend.MACD(close=df['Stock_Price']).macd()
df['MACD_Signal'] = ta.trend.MACD(close=df['Stock_Price']).macd_signal()

# คำนวณ RSI (Relative Strength Index)
df['RSI'] = ta.momentum.RSIIndicator(close=df['Stock_Price'], window=14, fillna=True).rsi()

df[['SMA_50', 'SMA_200', 'EMA_50', 'MACD', 'MACD_Signal', 'RSI']] = df[['SMA_50', 'SMA_200', 'EMA_50', 'MACD', 'MACD_Signal', 'RSI']].fillna(0)


# ตรวจสอบค่าที่ได้
print(df[['Stock_Price', 'SMA_50', 'SMA_200', 'EMA_50', 'MACD', 'MACD_Signal', 'RSI']].head(60))  # ดูค่า 60 วันแรก

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)  # ใช้ 5000 คำที่พบบ่อยสุด
X_news = vectorizer.fit_transform(df['Cleaned_News']).toarray()  # แปลงข้อความเป็นตัวเลข

print("Shape of X_news:", X_news.shape)  # ตรวจสอบขนาดของข้อมูล

print("Shape of X_news:", X_news.shape)

print(df.columns.tolist())  # ตรวจสอบว่ามีคอลัมน์ 'Cleaned_News' หรือไม่

"""รวม Sentiment + Technical Indicators กับ TF-IDF"""

import numpy as np

# คำนวณ Sentiment Score
from textblob import TextBlob
df['Sentiment'] = df['Cleaned_News'].fillna("").apply(lambda x: TextBlob(x).sentiment.polarity)

# เติมค่า NaN ใน Sentiment และ Indicators
df[['Sentiment', 'SMA_50', 'RSI', 'MACD', 'MACD_Signal']] = df[['Sentiment', 'SMA_50', 'RSI', 'MACD', 'MACD_Signal']].fillna(0)

# ถ้าขนาดไม่เท่ากัน ให้ตัด df ให้เท่ากับ X_news
df = df.iloc[:X_news.shape[0]]

# รวมข้อมูลทั้งหมด
X_final = np.hstack((X_news, df[['Sentiment', 'SMA_50', 'RSI', 'MACD', 'MACD_Signal']].values))

print("Final Feature Shape:", X_final.shape)  # ควรเป็น (จำนวนแถว, จำนวนฟีเจอร์ทั้งหมด)

"""เทรนโมเดล Machine Learning ใหม่"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# แบ่งข้อมูล Train/Test
X_train, X_test, y_train, y_test = train_test_split(X_final, df['Market_Movement'], test_size=0.2, random_state=42)

# เทรน Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)

# ทดสอบโมเดล
y_pred = model.predict(X_test)
print("New Accuracy (Logistic Regression):", accuracy_score(y_test, y_pred))

model = LogisticRegression(max_iter=500)

"""ใช้ StandardScaler() ปรับสเกลข้อมูล"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_final_scaled = scaler.fit_transform(X_final)

X_train, X_test, y_train, y_test = train_test_split(X_final_scaled, df['Market_Movement'], test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("New Accuracy (Logistic Regression after Scaling):", accuracy_score(y_test, y_pred))

model = LogisticRegression(max_iter=10000, solver='saga')

from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# Train XGBoost
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

# ตรวจสอบความแม่นยำ
print("Random Forest Accuracy:", accuracy_score(y_test, rf_preds))
print("XGBoost Accuracy:", accuracy_score(y_test, xgb_preds))

"""# Neural Network

**ทดสอบครั้งแรก**
"""

#  ใช้ LSTM (Long Short-Term Memory) เพราะเป็นโมเดลที่ดีมากกับ Sequential Data (ข้อมูลตามเวลา)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding

model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_text.shape[1]),
    LSTM(100, activation='relu'),
    Dense(1, activation='sigmoid')  # ใช้ 'softmax' ถ้ามีหลาย class
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

"""ใช้ Neural Network (MLP) แบบง่ายก่อน"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# ทำ Feature Scaling
scaler = StandardScaler()
X_final_scaled = scaler.fit_transform(X_final)

# แบ่งข้อมูล Train/Test
X_train, X_test, y_train, y_test = train_test_split(X_final_scaled, df['Market_Movement'], test_size=0.2, random_state=42)

# สร้างโมเดล Neural Network
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),  # ลด Overfitting
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # ใช้ Sigmoid สำหรับ Binary Classification
])

# คอมไพล์โมเดล
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# เทรนโมเดล
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# ประเมินผลโมเดล
loss, accuracy = model.evaluate(X_test, y_test)
print("Neural Network Accuracy:", accuracy)

""" ลองใช้ LSTM ถ้าข้อมูลข่าวเป็นข้อมูลต่อเนื่อง"""

from tensorflow.keras.layers import LSTM, Embedding

# สร้างโมเดล LSTM
model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_news.shape[1]),  # ใช้ Embedding กับข่าว
    LSTM(100, activation='relu', return_sequences=True),
    Dropout(0.3),
    LSTM(50, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# คอมไพล์โมเดล
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# เทรนโมเดล
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# ประเมินผลโมเดล
loss, accuracy = model.evaluate(X_test, y_test)
print("LSTM Neural Network Accuracy:", accuracy)

"""ใช้ Pretrained Word Embeddings (เช่น GloVe)
LSTM ของคุณใช้ Embedding Layer แบบ Train ใหม่ ซึ่งอาจไม่ได้เรียนรู้ความหมายของคำได้ดี
"""

import numpy as np
import gensim.downloader as api

# โหลด Word2Vec (Google News 300D)
word_vectors = api.load("word2vec-google-news-300")

# สร้าง Embedding Matrix
embedding_matrix = np.zeros((5000, 300))
for i, word in enumerate(vectorizer.get_feature_names_out()):
    if word in word_vectors:
        embedding_matrix[i] = word_vectors[word]

# ใช้ Pretrained Embedding
embedding_layer = Embedding(input_dim=5000, output_dim=300, weights=[embedding_matrix], trainable=False)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout

model = Sequential([
    Embedding(input_dim=5000, output_dim=128),
    Bidirectional(LSTM(100, activation='relu', return_sequences=True)),
    Dropout(0.3),  # ลด Overfitting
    LSTM(50, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # ใช้ Sigmoid สำหรับ Binary Classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))